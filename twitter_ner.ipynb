{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-nKga-mULzu",
        "outputId": "ef7b4a1d-79a7-4c76-93aa-6be75f99807e"
      },
      "outputs": [],
      "source": [
        "!pip install torch datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "E-96Fuh6UPQQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import BertForTokenClassification\n",
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "80f4yHsuUSEM"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "current_sentence = \"\"\n",
        "current_label = \"\"\n",
        "preprocessed = []\n",
        "punc = '''~!@#$%^&*()_+{}|:\"<>?`-[]\\;',./='''\n",
        "with open('train.csv', 'r') as csvfile:\n",
        "    csvreader = csv.reader(csvfile)\n",
        "    next(csvreader)  # Skip the header row\n",
        "    for row in csvreader:\n",
        "      data.append(row)\n",
        "for row in data:\n",
        "  word = ''\n",
        "  label = ''\n",
        "  if(row):\n",
        "    word = row[1]\n",
        "\n",
        "    label = row[2]\n",
        "  if word == '':\n",
        "    preprocessed.append([current_sentence[:-1], current_label[:-1]])\n",
        "    current_sentence = \"\"\n",
        "    current_label = \"\"\n",
        "  else:\n",
        "    if(len(word)>1):\n",
        "      for ele in word:\n",
        "        if ele in punc:\n",
        "          if(len(word)>1):\n",
        "            word = word.replace(ele, \"\",1)\n",
        "    current_sentence = current_sentence + word + \" \"\n",
        "    current_label = current_label + label + \" \"\n",
        "preprocessed.append([current_sentence[:-1], current_label[:-1]])\n",
        "filename = \"train1.csv\"\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(filename, \"w\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow([\"text\", \"labels\"])\n",
        "\n",
        "    # Write the data rows\n",
        "    writer.writerows(preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "KbTYBtIkUVs5"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "current_sentence = \"\"\n",
        "current_label = \"\"\n",
        "preprocessed = []\n",
        "punc = '''~!@#$%^&*()_+{}|:\"<>?`-[]\\;',./='''\n",
        "with open('validation.csv', 'r') as csvfile:\n",
        "    csvreader = csv.reader(csvfile)\n",
        "    next(csvreader)  # Skip the header row\n",
        "    for row in csvreader:\n",
        "      data.append(row)\n",
        "for row in data:\n",
        "  word = ''\n",
        "  label = ''\n",
        "  if(row):\n",
        "    word = row[1]\n",
        "    label = row[2]\n",
        "  if word == '':\n",
        "    preprocessed.append([current_sentence[:-1], current_label[:-1]])\n",
        "    current_sentence = \"\"\n",
        "    current_label = \"\"\n",
        "  else:\n",
        "    if(len(word)>1):\n",
        "      for ele in word:\n",
        "        if ele in punc:\n",
        "          if(len(word)>1):\n",
        "            word = word.replace(ele, \"\",1)\n",
        "    current_sentence = current_sentence + word + \" \"\n",
        "    current_label = current_label + label + \" \"\n",
        "preprocessed.append([current_sentence[:-1], current_label[:-1]])\n",
        "filename = \"validation1.csv\"\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(filename, \"w\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow([\"text\", \"labels\"])\n",
        "\n",
        "    # Write the data rows\n",
        "    writer.writerows(preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "TxnvGJNQUZF1"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train1.csv')\n",
        "# train = pd.read_csv('ner.csv')\n",
        "validation = pd.read_csv('validation1.csv')\n",
        "labels = [i.split() for i in train['labels'].values.tolist()]\n",
        "\n",
        "# Check how many labels are there in the dataset\n",
        "unique_labels = set()\n",
        "\n",
        "for lb in labels:\n",
        "  [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
        "\n",
        "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
        "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "9wY8rgGIUt4k"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "OP-dyqTeUwb_"
      },
      "outputs": [],
      "source": [
        "def align_label(texts, labels):\n",
        "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
        "\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "\n",
        "        elif word_idx != previous_word_idx:\n",
        "            try:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        else:\n",
        "            try:\n",
        "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    return label_ids\n",
        "\n",
        "class DataSequence(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        lb = [i.split() for i in df['labels'].values.tolist()]\n",
        "        txt = df['text'].values.tolist()\n",
        "        self.texts = [tokenizer(str(i),\n",
        "                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n",
        "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_data(self, idx):\n",
        "\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "\n",
        "        return torch.LongTensor(self.labels[idx])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_data = self.get_batch_data(idx)\n",
        "        batch_labels = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "4W4CnyzyU76v"
      },
      "outputs": [],
      "source": [
        "df_train = train\n",
        "df_val = validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ForbMm3SU9LB"
      },
      "outputs": [],
      "source": [
        "class BertModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(BertModel, self).__init__()\n",
        "\n",
        "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(unique_labels))\n",
        "    def forward(self, input_id, mask, label):\n",
        "\n",
        "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31ZxQdaRSU8x",
        "outputId": "7e528af3-9a30-44b4-f1e8-d6f16e5fb876"
      },
      "outputs": [],
      "source": [
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "EUufKbaTU-hR"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, df_train, df_val):\n",
        "\n",
        "    train_dataset = DataSequence(df_train)\n",
        "    val_dataset = DataSequence(df_val)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, num_workers=10, worker_init_fn=seed_worker, generator=g, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = DataLoader(val_dataset, num_workers=10, worker_init_fn=seed_worker, generator=g, batch_size=BATCH_SIZE)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    for epoch_num in range(EPOCHS):\n",
        "\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for train_data, train_label in tqdm(train_dataloader):\n",
        "\n",
        "            train_label = train_label.to(device)\n",
        "            mask = train_data['attention_mask'].squeeze(1).to(device)\n",
        "            input_id = train_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, logits = model(input_id, mask, train_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][train_label[i] != -100]\n",
        "              label_clean = train_label[i][train_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_train += acc\n",
        "              total_loss_train += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        for val_data, val_label in val_dataloader:\n",
        "\n",
        "            val_label = val_label.to(device)\n",
        "            mask = val_data['attention_mask'].squeeze(1).to(device)\n",
        "            input_id = val_data['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            loss, logits = model(input_id, mask, val_label)\n",
        "\n",
        "            for i in range(logits.shape[0]):\n",
        "\n",
        "              logits_clean = logits[i][val_label[i] != -100]\n",
        "              label_clean = val_label[i][val_label[i] != -100]\n",
        "\n",
        "              predictions = logits_clean.argmax(dim=1)\n",
        "              acc = (predictions == label_clean).float().mean()\n",
        "              total_acc_val += acc\n",
        "              total_loss_val += loss.item()\n",
        "\n",
        "        # target = 0.96\n",
        "        # if(total_acc_val / len(df_val) > target):\n",
        "        #   torch.save(model.state_dict(), \"/content/drive/My Drive/twitter_modelv2\")\n",
        "        #   target = val_loss\n",
        "        print(\n",
        "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_8ZYmeJVDwz",
        "outputId": "b4a58dc5-efa2-464f-fd0b-77282c3b5b82"
      },
      "outputs": [],
      "source": [
        "model = BertModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FkE8b80bOf5T"
      },
      "outputs": [],
      "source": [
        "# model = BertModel()\n",
        "# model.load_state_dict(torch.load(\"/content/drive/My Drive/twitter_best_0.36439\"))\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "aDHRP-wCVE5z",
        "outputId": "4b4270f4-52c0-41cb-827c-0a9ce11e9bdc"
      },
      "outputs": [],
      "source": [
        "# ~0.34+\n",
        "LEARNING_RATE = 0.005\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 6\n",
        "model.train()\n",
        "train_loop(model, df_train, df_val)\n",
        "# ~0.29+\n",
        "# LEARNING_RATE = 0.005\n",
        "# EPOCHS = 20\n",
        "# BATCH_SIZE = 6\n",
        "# model.train()\n",
        "# train_loop(model, df_train, df_val)\n",
        "# ~0.24+\n",
        "# LEARNING_RATE = 0.005\n",
        "# EPOCHS = 10\n",
        "# BATCH_SIZE = 6\n",
        "# model.train()\n",
        "# train_loop(model, df_train, df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "HWSBXyNnVLBe"
      },
      "outputs": [],
      "source": [
        "sentence_length = []\n",
        "count = 0\n",
        "data = []\n",
        "current_sentence = \"\"\n",
        "preprocessed = []\n",
        "label_all_tokens = False\n",
        "# punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "punc = '''~!@#$%^&*()_+{}|:\"<>?`-[]\\;',./='''\n",
        "with open('test_noans.csv', 'r') as csvfile:\n",
        "  csvreader = csv.reader(csvfile)\n",
        "  next(csvreader)  # Skip the header row\n",
        "  for row in csvreader:\n",
        "    data.append(row)\n",
        "for row in data:\n",
        "  word = ''\n",
        "  if(row):\n",
        "    word = row[1]\n",
        "  if word == '':\n",
        "    preprocessed.append([current_sentence[:-1]])\n",
        "    sentence_length.append(count)\n",
        "    current_sentence = \"\"\n",
        "    count = 0\n",
        "  else:\n",
        "    if(len(word)>1):\n",
        "      for ele in word:\n",
        "        if ele in punc:\n",
        "          if(len(word)>1):\n",
        "            word = word.replace(ele, \"\",1)\n",
        "      # word = re.sub(r'[^\\w\\s]', '', word)\n",
        "    current_sentence = current_sentence + word + \" \"\n",
        "    count +=1\n",
        "preprocessed.append([current_sentence[:-1]])\n",
        "sentence_length.append(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "56oJC-oDX-iQ"
      },
      "outputs": [],
      "source": [
        "def align_word_ids(texts):\n",
        "\n",
        "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
        "\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "\n",
        "    for word_idx in word_ids:\n",
        "\n",
        "        if word_idx is None:\n",
        "            label_ids.append(-100)\n",
        "\n",
        "        elif word_idx != previous_word_idx:\n",
        "            try:\n",
        "                label_ids.append(1)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        else:\n",
        "            try:\n",
        "                label_ids.append(1 if label_all_tokens else -100)\n",
        "            except:\n",
        "                label_ids.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "    return label_ids\n",
        "\n",
        "\n",
        "def evaluate_one_text(model, sentence):\n",
        "\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "\n",
        "    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    mask = text['attention_mask'].to(device)\n",
        "    input_id = text['input_ids'].to(device)\n",
        "    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)\n",
        "\n",
        "    logits = model(input_id, mask, None)\n",
        "    logits_clean = logits[0][label_ids != -100]\n",
        "\n",
        "    predictions = logits_clean.argmax(dim=1).tolist()\n",
        "\n",
        "    prediction_label = predictions\n",
        "    print(sentence)\n",
        "    print(prediction_label)\n",
        "    return prediction_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u05qTiyoX_DT",
        "outputId": "1a710d92-60a6-4318-c88b-855724868735"
      },
      "outputs": [],
      "source": [
        "test_ans = []\n",
        "\n",
        "j = 0\n",
        "for i in preprocessed:\n",
        "  test_ans = test_ans + evaluate_one_text(model, i)\n",
        "  j += 1\n",
        "len(test_ans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Qv2FQ4q0YAjY"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "j = 0\n",
        "for i in test_ans:\n",
        "  data.append([j,i])\n",
        "  j += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "kOk1_B1_YC5D"
      },
      "outputs": [],
      "source": [
        "filename = \"test_ans.csv\"\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(filename, \"w\", newline=\"\") as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writerow([\"id\",\"label\"])\n",
        "\n",
        "    # Write the data rows\n",
        "    writer.writerows(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0xyuxDAZILQ"
      },
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"/content/drive/My Drive/twitter_best_0.43111\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
